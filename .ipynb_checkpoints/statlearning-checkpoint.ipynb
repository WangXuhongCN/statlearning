{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statLearning project -- Cifar-10 image classfication\n",
    "\n",
    "*This project use cifar-10 as training set and the final result will be tested by new images from network.*\n",
    "\n",
    "This jupyter file contain two parts:\n",
    "\n",
    "- KNN algortithm from sklearn package\n",
    "- MLP by using TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded by \n",
    "\n",
    "\n",
    "*wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz*\n",
    "\n",
    "\n",
    "than unzip the dataset.\n",
    "\n",
    "You contents are like these:\n",
    "\n",
    "- ----cifar-10-batches-py\n",
    "- ----statlearning.ipynb\n",
    "- ----data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10,load_CIFAR_batch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'F:/course/统计学习方法/cifar-10-batches-py'\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subsample data for more dfficient code execution \n",
    "num_training = 5000\n",
    "#range(5)=[0,1,2,3,4]\n",
    "mask = range(num_training)\n",
    "x_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "num_test = 500\n",
    "mask = range(num_test)\n",
    "x_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the image data has three chanels\n",
    "#the next two step shape the image size 32*32*3 to 3072*1\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],-1))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],-1))\n",
    "print (\"after subsample and re shape:\")\n",
    "print ('x_train : ',X_train.shape,'x_test : ',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression() \n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 15\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / len(y_test)\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, len(y_test), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range = [3,8,15,20,30]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    neighbors.KNeighborsClassifier(), \n",
    "    X_train, y_train, \n",
    "    param_name=\"n_neighbors\", \n",
    "    param_range=param_range,\n",
    "    cv=3, \n",
    "    scoring=\"accuracy\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*this part using tensorflow to build up a mlp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "beginTime = time.time()\n",
    "\n",
    "# Parameter definitions\n",
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "max_steps = 1000\n",
    "\n",
    "# Uncommenting this line removes randomness\n",
    "# You'll get exactly the same result on each run\n",
    "# np.random.seed(1)\n",
    "def load_CIFAR10_batch(filename):\n",
    "  '''load data from single CIFAR-10 file'''\n",
    "\n",
    "  with open(filename, 'rb') as f:\n",
    "    if sys.version_info[0] < 3:\n",
    "      dict = pickle.load(f)\n",
    "    else:\n",
    "      dict = pickle.load(f, encoding='latin1')\n",
    "    x = dict['data']\n",
    "    y = dict['labels']\n",
    "    x = x.astype(float)\n",
    "    y = np.array(y)\n",
    "  return x, y\n",
    "def load_data():\n",
    "  '''load all CIFAR-10 data and merge training batches'''\n",
    "\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for i in range(1, 6):\n",
    "    filename = 'cifar-10-batches-py/data_batch_' + str(i)\n",
    "    X, Y = load_CIFAR10_batch(filename)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)\n",
    "\n",
    "  x_train = np.concatenate(xs)\n",
    "  y_train = np.concatenate(ys)\n",
    "  del xs, ys\n",
    "\n",
    "  x_test, y_test = load_CIFAR10_batch('cifar-10-batches-py/test_batch')\n",
    "\n",
    "  classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "    'ship', 'truck']\n",
    "\n",
    "  # Normalize Data\n",
    "  mean_image = np.mean(x_train, axis=0)\n",
    "  x_train -= mean_image\n",
    "  x_test -= mean_image\n",
    "\n",
    "  data_dict = {\n",
    "    'images_train': x_train,\n",
    "    'labels_train': y_train,\n",
    "    'images_test': x_test,\n",
    "    'labels_test': y_test,\n",
    "    'classes': classes\n",
    "  }\n",
    "  return data_dict\n",
    "# Prepare data\n",
    "data_sets = load_data()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Prepare the TensorFlow graph\n",
    "# (We're only defining the graph here, no actual calculations taking place)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Define input placeholders\n",
    "images_placeholder = tf.placeholder(tf.float32, shape=[None, 3072])\n",
    "labels_placeholder = tf.placeholder(tf.int64, shape=[None])\n",
    "\n",
    "# Define variables (these are the values we want to optimize)\n",
    "weights = tf.Variable(tf.zeros([3072, 10]))\n",
    "biases = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Define the classifier's result\n",
    "logits = tf.matmul(images_placeholder, weights) + biases\n",
    "\n",
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "  labels=labels_placeholder))\n",
    "\n",
    "# Define the training operation\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Operation comparing prediction with true label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), labels_placeholder)\n",
    "\n",
    "# Operation calculating the accuracy of our predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the TensorFlow graph\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Initialize variables\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # Repeat max_steps times\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # Generate input data batch\n",
    "    indices = np.random.choice(data_sets['images_train'].shape[0], batch_size)\n",
    "    images_batch = data_sets['images_train'][indices]\n",
    "    labels_batch = data_sets['labels_train'][indices]\n",
    "\n",
    "    # Periodically print out the model's current accuracy\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = sess.run(accuracy, feed_dict={\n",
    "        images_placeholder: images_batch, labels_placeholder: labels_batch})\n",
    "      print('Step {:5d}: training accuracy {:g}'.format(i, train_accuracy))\n",
    "\n",
    "    # Perform a single training step\n",
    "    sess.run(train_step, feed_dict={images_placeholder: images_batch,\n",
    "      labels_placeholder: labels_batch})\n",
    "\n",
    "  # After finishing the training, evaluate on the test set\n",
    "  test_accuracy = sess.run(accuracy, feed_dict={\n",
    "    images_placeholder: data_sets['images_test'],\n",
    "    labels_placeholder: data_sets['labels_test']})\n",
    "  print('Test accuracy {:g}'.format(test_accuracy))\n",
    "\n",
    "endTime = time.time()\n",
    "print('Total time: {:5.2f}s'.format(endTime - beginTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
